{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBT Tutorial 101 - Recommender System Lab\n",
    "\n",
    "This notebook demonstrates two approaches to building a recommendation system using the data transformed by dbt:\n",
    "\n",
    "1. **Weighted Hybrid Recommender**: Combines Collaborative Filtering, Market Basket Analysis, and Content-Based Filtering.\n",
    "2. **Gradient Boosting Ranker**: A 'Learning to Rank' approach that predicts the probability of purchase based on context.\n",
    "\n",
    "## Prerequisites\n",
    "Ensure you have run `dbt run` so that `raw.fact_order_items` and `raw.fact_orders` exist in your local Postgres.",
    "\n    3. **Neural Collaborative Filtering**: Deep Learning approach using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Setup logging to show progress\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Database Configuration\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"mysecretpassword\",\n",
    "    \"schema\": \"raw\"\n",
    "}\n",
    "\n",
    "def get_db_connection():\n",
    "    conn_str = f\"postgresql+psycopg2://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "    return create_engine(conn_str)\n",
    "\n",
    "engine = get_db_connection()\n",
    "print(\"Database connection established.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Weighted Hybrid Recommender\n",
    "\n",
    "We will load the `fact_order_items` table and build 4 distinct recommendation models:\n",
    "1. **Popularity**: Baseline.\n",
    "2. **Collaborative Filtering (CF)**: Item-Item Cosine Similarity.\n",
    "3. **Association Rules**: Apriori algorithm for Market Basket Analysis.\n",
    "4. **Content-Based**: Metadata similarity (Price, Type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Interaction Data\n",
    "query = f\"SELECT * FROM {DB_CONFIG['schema']}.fact_order_items\"\n",
    "df_items = pd.read_sql(query, engine)\n",
    "print(f\"Loaded {len(df_items)} rows.\")\n",
    "df_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedHybridRecommender:\n",
    "    def __init__(self, df, weights={'cf': 0.4, 'content': 0.3, 'rules': 0.2, 'pop': 0.1}):\n",
    "        self.df = df\n",
    "        self.weights = weights\n",
    "        \n",
    "        # Models\n",
    "        self.basket_encoded = None\n",
    "        self.item_sim_df = None # CF\n",
    "        self.content_sim_df = None # Content\n",
    "        self.rules = None # Rules\n",
    "        self.global_pop_score = None # Popularity\n",
    "        \n",
    "        self._train_models()\n",
    "        \n",
    "    def _train_models(self):\n",
    "        print(\"Training component models...\")\n",
    "        \n",
    "        # 1. Global Popularity\n",
    "        pop_counts = self.df['product_name'].value_counts()\n",
    "        self.global_pop_score = (pop_counts - pop_counts.min()) / (pop_counts.max() - pop_counts.min())\n",
    "        \n",
    "        # 2. Collaborative Filtering (Item-Item)\n",
    "        basket = self.df.groupby(['order_id', 'product_name'])['product_name'].count().unstack().reset_index().fillna(0).set_index('order_id')\n",
    "        self.basket_encoded = basket.map(lambda x: 1 if x >= 1 else 0)\n",
    "        \n",
    "        item_user_matrix = self.basket_encoded.T\n",
    "        sim_matrix = cosine_similarity(item_user_matrix)\n",
    "        self.item_sim_df = pd.DataFrame(sim_matrix, index=self.basket_encoded.columns, columns=self.basket_encoded.columns)\n",
    "        \n",
    "        # 3. Association Rules\n",
    "        frequent_itemsets = apriori(self.basket_encoded.astype(bool), min_support=0.005, use_colnames=True)\n",
    "        if not frequent_itemsets.empty:\n",
    "            self.rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "        else:\n",
    "            self.rules = pd.DataFrame()\n",
    "            \n",
    "        # 4. Content Based\n",
    "        product_features = self.df[['product_name', 'product_price', 'is_food_item', 'is_drink_item']].drop_duplicates('product_name').set_index('product_name')\n",
    "        product_features['product_price'] = (product_features['product_price'] - product_features['product_price'].mean()) / product_features['product_price'].std()\n",
    "        content_sim = cosine_similarity(product_features.fillna(0))\n",
    "        self.content_sim_df = pd.DataFrame(content_sim, index=product_features.index, columns=product_features.index)\n",
    "        \n",
    "        print(\"Training complete.\")\n",
    "\n",
    "    def recommend(self, product_name=None, basket_items=None, top_n=5):\n",
    "        all_products = self.global_pop_score.index.tolist()\n",
    "        scores = {p: 0.0 for p in all_products}\n",
    "        \n",
    "        # 1. Popularity\n",
    "        w_pop = self.weights.get('pop', 0.1)\n",
    "        for p, score in self.global_pop_score.items():\n",
    "            scores[p] += score * w_pop\n",
    "            \n",
    "        # 2. CF\n",
    "        if product_name and product_name in self.item_sim_df.index:\n",
    "            w_cf = self.weights.get('cf', 0.4)\n",
    "            sim_scores = self.item_sim_df[product_name]\n",
    "            for p, score in sim_scores.items():\n",
    "                scores[p] += score * w_cf\n",
    "                \n",
    "        # 3. Content\n",
    "        if product_name and product_name in self.content_sim_df.index:\n",
    "            w_content = self.weights.get('content', 0.3)\n",
    "            sim_scores = self.content_sim_df[product_name]\n",
    "            for p, score in sim_scores.items():\n",
    "                scores[p] += score * w_content\n",
    "                \n",
    "        # 4. Rules\n",
    "        if basket_items and not self.rules.empty:\n",
    "            w_rules = self.weights.get('rules', 0.2)\n",
    "            basket_set = set(basket_items)\n",
    "            relevant_rules = self.rules[self.rules['antecedents'].apply(lambda x: x.issubset(basket_set))]\n",
    "            for _, row in relevant_rules.iterrows():\n",
    "                for item in list(row['consequents']):\n",
    "                    scores[item] += row['confidence'] * w_rules\n",
    "\n",
    "        # Filter self\n",
    "        exclude = set()\n",
    "        if product_name: exclude.add(product_name)\n",
    "        if basket_items: exclude.update(basket_items)\n",
    "        \n",
    "        final_scores = {k: v for k, v in scores.items() if k not in exclude}\n",
    "        ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ranked[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize User-Defined Weights\n",
    "weights = {'cf': 0.4, 'content': 0.2, 'rules': 0.3, 'pop': 0.1}\n",
    "recommender = WeightedHybridRecommender(df_items, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1: Viewing 'doctor stew'\n",
    "print(\"Recommendations for 'doctor stew':\")\n",
    "recs = recommender.recommend(product_name=\"doctor stew\", basket_items=[])\n",
    "pd.DataFrame(recs, columns=['Product', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2: Basket contains 'mel-bun'\n",
    "print(\"Recommendations for Basket=['mel-bun']:\")\n",
    "recs = recommender.recommend(product_name=None, basket_items=['mel-bun'])\n",
    "pd.DataFrame(recs, columns=['Product', 'Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient Boosting Ranker\n",
    "\n",
    "In this section, we treat recommendation as a binary classification problem: \"Will the user buy this item given the context?\"\n",
    "\n",
    "We need to generate **Negative Samples** (items the user *didn't* buy) to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data for Grading Boosting...\")\n",
    "# Fetch Order Items + Context (Time)\n",
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        i.order_id,\n",
    "        i.product_id,\n",
    "        i.product_price,\n",
    "        i.is_food_item,\n",
    "        i.is_drink_item,\n",
    "        o.ordered_at\n",
    "    FROM {DB_CONFIG['schema']}.fact_order_items i\n",
    "    JOIN {DB_CONFIG['schema']}.fact_orders o ON i.order_id = o.order_id\n",
    "    LIMIT 20000\n",
    "\"\"\"\n",
    "df_pos = pd.read_sql(query, engine)\n",
    "df_pos['target'] = 1\n",
    "\n",
    "print(f\"Loaded {len(df_pos)} positive samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Negative Samples\n",
    "unique_products = df_pos[['product_id', 'product_price', 'is_food_item', 'is_drink_item']].drop_duplicates()\n",
    "all_product_ids = unique_products['product_id'].values\n",
    "purchased_map = df_pos.groupby('order_id')['product_id'].apply(set).to_dict()\n",
    "orders_unique = df_pos[['order_id', 'ordered_at']].drop_duplicates()\n",
    "\n",
    "neg_samples = []\n",
    "for _, row in orders_unique.iterrows():\n",
    "    oid = row['order_id']\n",
    "    bought_items = purchased_map.get(oid, set())\n",
    "    candidates = [p for p in all_product_ids if p not in bought_items]\n",
    "    \n",
    "    if candidates:\n",
    "        chosen_neg = np.random.choice(candidates, size=min(len(candidates), 2), replace=False)\n",
    "        for neg_pid in chosen_neg:\n",
    "            prod_info = unique_products[unique_products['product_id'] == neg_pid].iloc[0]\n",
    "            neg_samples.append({\n",
    "                'order_id': oid,\n",
    "                'product_id': neg_pid,\n",
    "                'product_price': prod_info['product_price'],\n",
    "                'is_food_item': prod_info['is_food_item'],\n",
    "                'is_drink_item': prod_info['is_drink_item'],\n",
    "                'ordered_at': row['ordered_at'],\n",
    "                'target': 0\n",
    "            })\n",
    "            \n",
    "df_neg = pd.DataFrame(neg_samples)\n",
    "df_full = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "print(f\"Total dataset size: {len(df_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "df_full['ordered_at'] = pd.to_datetime(df_full['ordered_at'])\n",
    "df_full['hour_of_day'] = df_full['ordered_at'].dt.hour\n",
    "df_full['day_of_week'] = df_full['ordered_at'].dt.dayofweek\n",
    "df_full['is_weekend'] = df_full['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "feature_cols = ['product_price', 'is_food_item', 'is_drink_item', 'hour_of_day', 'day_of_week', 'is_weekend']\n",
    "X = df_full[feature_cols]\n",
    "y = df_full['target']\n",
    "\n",
    "# Handle types\n",
    "X['is_food_item'] = X['is_food_item'].astype(int)\n",
    "X['is_drink_item'] = X['is_drink_item'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting Classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(f\"Model Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Visualization\n",
    "importance = model.feature_importances_\n",
    "indices = np.argsort(importance)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance - Gradient Boosting\")\n",
    "plt.bar(range(X.shape[1]), importance[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), [feature_cols[i] for i in indices], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Neural Collaborative Filtering (Deep Learning)\n",
    "\n",
    "We will use **PyTorch** to build a Neural Collaborative Filtering (NCF) model. \n",
    "This model learns latent vector representations (embeddings) for Orders and Products to predict the likelihood of a purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# PyTorch Dataset\n",
    "class OrderItemDataset(Dataset):\n",
    "    def __init__(self, user_ids, item_ids, targets):\n",
    "        self.user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
    "        self.item_ids = torch.tensor(item_ids, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.item_ids[idx], self.targets[idx]\n",
    "\n",
    "# Neural Network Model\n",
    "class NCFModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=32):\n",
    "        super(NCFModel, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Simple MLP\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, user_idx, item_idx):\n",
    "        u_emb = self.user_embedding(user_idx)\n",
    "        i_emb = self.item_embedding(item_idx)\n",
    "        x = torch.cat([u_emb, i_emb], dim=1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation for Deep Learning\n",
    "\n",
    "# Re-load fresh data\n",
    "query = f\"SELECT order_id, product_name FROM {DB_CONFIG['schema']}.fact_order_items\"\n",
    "df_dl = pd.read_sql(query, engine)\n",
    "\n",
    "# Encode IDs\n",
    "order_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "df_dl['order_idx'] = order_encoder.fit_transform(df_dl['order_id'])\n",
    "df_dl['item_idx'] = item_encoder.fit_transform(df_dl['product_name'])\n",
    "\n",
    "num_orders = len(order_encoder.classes_)\n",
    "num_items = len(item_encoder.classes_)\n",
    "print(f\"Deep Learning Input: {num_orders} Orders, {num_items} Items\")\n",
    "\n",
    "# Create Samples (Positive + Negative)\n",
    "user_ids = df_dl['order_idx'].values\n",
    "item_ids = df_dl['item_idx'].values\n",
    "targets = np.ones(len(df_dl))\n",
    "\n",
    "# Negative Sampling (1:1)\n",
    "neg_user_ids = np.random.randint(0, num_orders, len(df_dl))\n",
    "neg_item_ids = np.random.randint(0, num_items, len(df_dl))\n",
    "neg_targets = np.zeros(len(df_dl))\n",
    "\n",
    "all_users = np.concatenate([user_ids, neg_user_ids])\n",
    "all_items = np.concatenate([item_ids, neg_item_ids])\n",
    "all_targets = np.concatenate([targets, neg_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Loop\n",
    "dataset = OrderItemDataset(all_users, all_items, all_targets)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model_ncf = NCFModel(num_orders, num_items, embedding_dim=32)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_ncf.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training Neural Network...\")\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for u, i, t in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model_ncf(u, i).squeeze()\n",
    "        loss = criterion(predictions, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Learned Embeddings Similarity\n",
    "model_ncf.eval()\n",
    "with torch.no_grad():\n",
    "    item_embeddings = model_ncf.item_embedding.weight.data.numpy()\n",
    "\n",
    "def get_neural_recs(product_name, top_n=5):\n",
    "    if product_name not in item_encoder.classes_:\n",
    "        return []\n",
    "    \n",
    "    idx = item_encoder.transform([product_name])[0]\n",
    "    target_emb = item_embeddings[idx]\n",
    "    \n",
    "    # Cosine Sim\n",
    "    norms = np.linalg.norm(item_embeddings, axis=1)\n",
    "    dot_products = np.dot(item_embeddings, target_emb)\n",
    "    sims = dot_products / (norms * np.linalg.norm(target_emb))\n",
    "    \n",
    "    top_indices = np.argsort(sims)[::-1][1:top_n+1]\n",
    "    return [(item_encoder.inverse_transform([i])[0], sims[i]) for i in top_indices]\n",
    "\n",
    "print(\"Neural Recommendations for 'doctor stew':\")\n",
    "for item, score in get_neural_recs('doctor stew'):\n",
    "    print(f\"{item} (Sim: {score:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}